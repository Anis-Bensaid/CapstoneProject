{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To be run only once\n",
    "# if 0 == 1:\n",
    "#     !pip install gensim\n",
    "#     !pip install PyLDAvis\n",
    "#     !pip install spacy\n",
    "#     !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import en_core_web_sm\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asaid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_drive_path = \"C:/Users/cfowle/The Estée Lauder Companies Inc/TeamAnis - General/\"\n",
    "one_drive_path = \"C:/Users/asaid/The Estée Lauder Companies Inc/TeamAnis - General/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.read_pickle('reviews.pickle')\n",
    "\n",
    "# cols = ['type', 'onlinepost_id', 'source_product_identifier', 'onlinestatement_id',\n",
    "#        'date', 'title', 'description', 'geography', 'channel', 'product_id',\n",
    "#        'rating', 'sentiment']\n",
    "# reviews = reviews[cols]\n",
    "\n",
    "# reviews.loc[reviews['type']=='Cosmetics','type']='temps'\n",
    "# reviews.loc[reviews['type']=='Skincare','type']='Cosmetics'\n",
    "# reviews.loc[reviews['type']=='temps','type']='Skincare'\n",
    "\n",
    "# reviews = reviews[reviews['geography']=='USA']\n",
    "# reviews.drop(columns=['geography'], inplace=True)\n",
    "\n",
    "# reviews.to_pickle('reviews_filtered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.read_pickle('reviews_filtered.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating date columns in the right dtype and dropping the day of the month: 2019-02-24 => 2019-02-01\n",
    "# reviews.loc[:,'clean_date'] = pd.to_datetime(reviews['date'], errors='coerce')\n",
    "# if reviews['clean_date'].isna().sum() > 0:\n",
    "#     print('{} rows have been dropped because the date format is wrong.'.format(reviews['clean_date'].isna().sum()))\n",
    "#     display(reviews.loc[reviews['clean_date'].isna(), 'date'])\n",
    "#     reviews = reviews.dropna(subset='date')\n",
    "# reviews['date'] = reviews['clean_date']  \n",
    "# reviews = reviews.drop('clean_date', axis=1)\n",
    "# reviews['date'] = reviews['date'].dt.to_period('m')\n",
    "\n",
    "# # Checking for missing data (NA => -1)\n",
    "# if reviews['rating'].isna().sum()>0:\n",
    "#     print('{} rows are missing ratings'.format(reviews['rating'].isna().sum()))\n",
    "#     reviews.loc[:,'rating'] = reviews['rating'].fillna(-1).astype(int)\n",
    "\n",
    "# if reviews['sentiment'].isna().sum()>0:\n",
    "#     print('{} rows are missing sentiments'.format(reviews['sentiment'].isna().sum()))\n",
    "#     reviews.loc[:,'sentiment'] = reviews['sentiment'].fillna(-1).astype(int)\n",
    "    \n",
    "# # Transforming rating and sentiment to dummy variables (one-hot encoding)\n",
    "# reviews.loc[:,'sentiment'] = reviews['sentiment'].str.lower()\n",
    "# reviews.loc[:,'rating'] = reviews['rating'].astype(int)\n",
    "# reviews = pd.concat([reviews, pd.get_dummies(data=reviews[['rating','sentiment']], columns=['rating','sentiment'], dtype=int)], axis=1)\n",
    "\n",
    "# # Readding NAs data to ratings\n",
    "# reviews.loc[reviews['rating']==-1,'rating'] = np.nan\n",
    "# reviews.loc[reviews['sentiment']==-1,'sentiment'] = np.nan\n",
    "\n",
    "# # Transforming sentiment to integer data (positive:1; netural:0, negative:-1)\n",
    "# reviews.loc[:,'sentiment'] = reviews['sentiment_positive'] - reviews['sentiment_negative']\n",
    "\n",
    "# # Aggregating RR data by OnlinePost_ID\n",
    "\n",
    "# # Creating a column to count the number of statements by review once aggreagtion happens\n",
    "# reviews['nb_statements'] = reviews['sentiment']\n",
    "\n",
    "# reviews = reviews.groupby(['type',\n",
    "#                            'channel',\n",
    "#                            'source_product_identifier',  \n",
    "#                            'date',\n",
    "#                            'onlinepost_id']).agg({\n",
    "#     'description': lambda x:'. '.join(list(x)),\n",
    "#     'nb_statements':'count',\n",
    "#     'rating':'first',\n",
    "#     'rating_1':'first',\n",
    "#     'rating_2':'first',\n",
    "#     'rating_3':'first',\n",
    "#     'rating_4':'first',\n",
    "#     'rating_5':'first',\n",
    "#     'sentiment_negative':'sum',\n",
    "#     'sentiment_neutral':'sum',\n",
    "#     'sentiment_positive':'sum',\n",
    "#     'sentiment':'mean'\n",
    "# }).reset_index()\n",
    "\n",
    "# # Normalize the one hot sentiment encoding counts (sentiment_negative, sentiment_neutral, sentiment_positive) by the nb_statement.\n",
    "# reviews[['sentiment_negative', 'sentiment_neutral', 'sentiment_positive']] = reviews[['sentiment_negative', 'sentiment_neutral', 'sentiment_positive']].div(reviews['nb_statements'], axis=0)\n",
    "\n",
    "# reviews.to_pickle('reviews_wrangled.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_pickle('reviews_wrangled.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding product related words to the stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from'])\n",
    "\n",
    "brands = pd.read_csv('elc_brands.csv', encoding='ISO-8859-1')\n",
    "catalogue = pd.read_csv('elc_catalogue.csv', encoding='ISO-8859-1')\n",
    "\n",
    "stop_words.extend(list(set(str(w).lower() for w in ' '.join(brands['ELC_Brand'].unique().tolist() +\n",
    "                                          catalogue['Major_Category'].unique().tolist() +\n",
    "                                          catalogue['Application'].unique().tolist() +\n",
    "                                          catalogue['Category'].unique().tolist() +\n",
    "                                          catalogue['SubCategory'].unique().tolist() \n",
    "                                         ).replace('/', ' ').split())))\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "stop_words_nlp = nlp(' '.join([' '.join(gensim.utils.simple_preprocess(str(word), deacc=True)) for word in stop_words]))\n",
    "stop_words = [token.lemma_ for token in stop_words_nlp]\n",
    "\n",
    "\n",
    "# Cache stop_words into hash\n",
    "stop_words = Counter(stop_words)\n",
    "\n",
    "\n",
    "def preprocess(sentences, stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(' '.join([token for token in gensim.utils.simple_preprocess(str(sentence), deacc=True)]) )\n",
    "        yield([token.lemma_ for token in doc if (token.pos_ in allowed_postags) and (not (token in stop_words))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████████▌                                                            | 631921/4636608 [25:38<2:59:26, 371.96it/s]"
     ]
    }
   ],
   "source": [
    "reviews['tokens'] = list(tqdm.tqdm(preprocess(reviews['description'].values.tolist(), stop_words), position=0, leave=True, total=len(reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_pickle('reviews_wrangled_w_tokens.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_pickle('reviews_wrangled_w_tokens.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = pd.read_csv('reviews_products.csv', low_memory=False)\n",
    "products = rp[['type', 'channel', 'source_product_identifier', 'product', 'brand_abbrev', 'elc_brand',\n",
    "       'brand_score', 'item_description', 'product_score', 'itemid_4',\n",
    "       'major_category_id', 'major_category', 'application_id', 'application',\n",
    "       'category_id', 'category', 'sub_category_id', 'sub_category']].drop_duplicates()\n",
    "products = products.dropna(subset=['elc_brand'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews  = reviews.merge(products, left_on=['type', 'channel', 'source_product_identifier'], right_on=['type', 'channel', 'source_product_identifier'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_pickle('reviews_products.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.read_pickle('reviews_products.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModeller():\n",
    "    def __init__(self, df, column):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.column = column\n",
    "        self.docs = df[column].values.tolist()\n",
    "    \n",
    "    def compute_coherence_values(self, corpus, id2word, text, k, a, b):\n",
    "\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=k, \n",
    "                                               random_state=3,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               alpha=a,\n",
    "                                               eta=b)\n",
    "\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=id2word, coherence='c_v')\n",
    "\n",
    "        return coherence_model_lda.get_coherence()\n",
    "    \n",
    "    def train_valid_lda(self,\n",
    "                        nb_samples: int = 100000,\n",
    "                        topics_range: list = [3,5,7,9],\n",
    "                        alpha: list = [0.1, 0.01, 'symmetric','asymmetric'],\n",
    "                        beta: list = [0.1, 'symmetric']                        \n",
    "                        ):\n",
    "        \n",
    "        nb_samples = min(nb_samples, len(self.docs))\n",
    "        \n",
    "        # Training set\n",
    "        train = random.sample(self.docs, nb_samples)\n",
    "        \n",
    "        # Create Dictionary\n",
    "        self.id2word = corpora.Dictionary(train)\n",
    "        \n",
    "        # Term Document Frequency\n",
    "        self.corpus = [self.id2word.doc2bow(text) for text in train]\n",
    "        \n",
    "        # Result dictionary\n",
    "        self.model_results = {'Topics': [],\n",
    "                         'Alpha': [],\n",
    "                         'Beta': [],\n",
    "                         'Coherence': []\n",
    "                        }\n",
    "\n",
    "        # Can take a long time to run\n",
    "        print('Validating hyperparameters...')\n",
    "        if 1 == 1:\n",
    "            pbar = tqdm.tqdm(total=(len(beta)*(len(alpha))*len(topics_range)), position=0, leave=True)\n",
    "            # iterate through number of topics\n",
    "            for k in topics_range:\n",
    "                # iterate through alpha values\n",
    "                for a in alpha:\n",
    "                    # iterare through beta values\n",
    "                    for b in beta:\n",
    "                        # get the coherence score for the given parameters\n",
    "                        cv = self.compute_coherence_values(corpus=self.corpus, id2word=self.id2word, text=train,\n",
    "                                                      k=k, a=a, b=b)\n",
    "                        # Save the model results\n",
    "                        self.model_results['Topics'].append(k)\n",
    "                        self.model_results['Alpha'].append(a)\n",
    "                        self.model_results['Beta'].append(b)\n",
    "                        self.model_results['Coherence'].append(cv)\n",
    "                        print('nb topics: {}, alpha: {}, beta: {}, coherence: {}'.format(k,a,b,cv))\n",
    "                        pbar.update(1)\n",
    "        self.cv_results = pd.DataFrame(self.model_results).sort_values(\"Coherence\", ascending=False)\n",
    "        self.cv_results.to_csv('lda_tuning_results.csv', index=False)\n",
    "        pbar.close()\n",
    "        \n",
    "        self.best_param = self.cv_results.iloc[0]\n",
    "        self.best_num_topics = int(self.best_param['Topics'])\n",
    "        self.best_alpha = self.best_param['Alpha']\n",
    "        self.best_beta = self.best_param['Beta']\n",
    "        name = 'cv_best'\n",
    "        \n",
    "        # Build LDA model\n",
    "        print('Training best model...')\n",
    "        self.lda_model = gensim.models.LdaMulticore(corpus=self.corpus,\n",
    "                                                   id2word=self.id2word,\n",
    "                                                   num_topics=self.best_num_topics,\n",
    "                                                   alpha = self.best_alpha,\n",
    "                                                   eta = self.best_beta,\n",
    "                                                   random_state=3,\n",
    "                                                   chunksize=100,\n",
    "                                                   passes=10,\n",
    "                                                   per_word_topics=True)\n",
    "        print('Done.')\n",
    "        pyLDAvis.enable_notebook()\n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(self.lda_model, self.corpus, self.id2word)\n",
    "        display(LDAvis_prepared)\n",
    "        pyLDAvis.save_html(LDAvis_prepared, name+'_'+str(self.best_alpha)+'_'+str(self.best_beta)+'_'+str(self.best_num_topics)+'.html')\n",
    "        self.lda_model.save(name+'_'+str(self.best_alpha)+'_'+str(self.best_beta)+'_'+str(self.best_num_topics))\n",
    "        pickle.dump(self.id2word, open( \"id2word_\"+name+'_'+str(self.best_alpha)+'_'+str(self.best_beta)+'_'+str(self.best_num_topics)+\".pickle\", \"wb\" ))\n",
    "        return self.lda_model, self.id2word\n",
    "        \n",
    "    def get_docs_topics(self):\n",
    "        # Term Document Frequency\n",
    "        corpus = [self.id2word.doc2bow(text) for text in self.docs]\n",
    "        self.output = pd.concat([self.df, pd.DataFrame(gensim.matutils.corpus2csc(self.lda_model.get_document_topics(corpus)).T.toarray(), columns=['topic_'+str(i) for i in range(1,self.best_num_topics+1)])], axis=1)\n",
    "        return self.output\n",
    "\n",
    "    def view(self):\n",
    "        pyLDAvis.enable_notebook()\n",
    "        self.LDAvis_prepared = pyLDAvis.gensim.prepare(self.lda_model, self.corpus, self.id2word)\n",
    "        display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_lda = TopicModeller(reviews, 'tokens')\n",
    "lda, id2word = all_lda.train_valid_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_display_save(tokens, nb_samples, num_topics, alpha, eta, name):\n",
    "    nb_samples = min(nb_samples, len(tokens))\n",
    "        \n",
    "    # Training set\n",
    "    train = tokens.sample(nb_samples, random_state=3).values.tolist()\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(train)\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in train]\n",
    "    start=time.time()\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=num_topics,\n",
    "                                                   alpha =alpha ,\n",
    "                                                   eta =eta ,\n",
    "                                                   random_state=3,\n",
    "                                                   chunksize=100,\n",
    "                                                   passes=10,\n",
    "                                                   per_word_topics=True)\n",
    "    pyLDAvis.enable_notebook()\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    display(LDAvis_prepared)\n",
    "    pyLDAvis.save_html(LDAvis_prepared, name+'_'+str(alpha)+'_'+str(eta)+'_'+str(num_topics)+'.html')\n",
    "    lda_model.save(name+'_'+str(alpha)+'_'+str(eta)+'_'+str(num_topics))\n",
    "    pickle.dump(id2word, open( \"id2word_\"+ name+'_'+str(alpha)+'_'+str(eta)+'_'+str(num_topics)+\".p\", \"wb\" ))\n",
    "    print(time.time()-start)\n",
    "    return lda_model, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda, id2word = train_display_save(reviews['tokens'], 500000, 8, alpha='asymmetric', eta='symmetric', name='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = reviews['tokens'].values.tolist()\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = gensim.matutils.corpus2csc(lda.get_document_topics(corpus)).T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.DataFrame(topics, columns=['topic_'+str(i) for i in range(1,topics.shape[1]+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([reviews, topics], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_pickle('reviews_w_topics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print('Topic #{}'.format(topic[0]+1))\n",
    "    print('Terms: ', topic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_display_save(reviews['tokens'], 100000, 10, alpha='asymmetric', eta='symmetric', name='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_display_save(reviews['tokens'], 100000, 9, alpha='asymmetric', eta='symmetric', name='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_display_save(reviews['tokens'], 100000, 8, alpha='asymmetric', eta='symmetric', name='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_display_save(reviews['tokens'], 100000, 7, alpha='asymmetric', eta='symmetric', name='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_display_save(reviews['tokens'], 100000, 6, alpha='asymmetric', eta='symmetric', name='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_display_save(reviews['tokens'], 100000, 5, alpha='asymmetric', eta='symmetric', name='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_display_save(reviews['tokens'], 100000, 4, alpha='asymmetric', eta=0.1, name='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_display_save(reviews['tokens'], 100000, 3, alpha='asymmetric', eta=0.1, name='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting by review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_reviews['tokens'].sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews_6 = train_display_save(tokens=good_reviews['tokens'], nb_samples=100000, num_topics=6, alpha='asymmetric', eta=0.1, name='good_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_reviews_6 = train_display_save(tokens=bad_reviews['tokens'], nb_samples=100000, num_topics=6, alpha='asymmetric', eta=0.1, name='bad_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_reviews_6 = train_display_save(tokens=neutral_reviews['tokens'], nb_samples=100000, num_topics=6, alpha='asymmetric', eta=0.1, name='neutral_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_display(self, title):\n",
    "    p = pyLDAvis.gensim.prepare(self.lda_model, self.corpus, self.id2word)\n",
    "    pyLDAvis.save_html(p, title+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_display(good_lda, 'good_reviews_8_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_display(bad_lda, 'bad_reviews_8_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_display(neutral_lda, 'neutral_reviews_8_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews = reviews[reviews['rating']>=4]\n",
    "neutral_reviews = reviews[(reviews['rating']>2) & (reviews['rating']<4)]\n",
    "bad_reviews = reviews[reviews['rating']<=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(good_reviews))\n",
    "good_lda = TopicModeller(good_reviews, 'tokens')\n",
    "good_lda.train_valid_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(good_lda.cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_lda.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bad_reviews))\n",
    "bad_lda = TopicModeller(bad_reviews, 'tokens')\n",
    "bad_lda.train_valid_lda()\n",
    "bad_lda.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bad_lda.cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_lda.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(neutral_reviews))\n",
    "neutral_lda = TopicModeller(neutral_reviews, 'tokens')\n",
    "neutral_lda.train_valid_lda()\n",
    "neutral_lda.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(neutral_lda.cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neutral_lda.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews = good_lda.get_docs_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_reviews = neutral_lda.get_docs_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_reviews = bad_lda.get_docs_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews.to_pickle('good_reviews.pickle')\n",
    "bad_reviews.to_pickle('bad_reviews.pickle')\n",
    "neutral_reviews.to_pickle('neutral_reviews.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_lda.cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_reviews_3 = train_display_save(tokens=good_reviews['tokens'], nb_samples=100000, num_topics=3, alpha='asymmetric', eta=0.1, name='good_reviews')\n",
    "bad_reviews_3 = train_display_save(tokens=bad_reviews['tokens'], nb_samples=100000, num_topics=3, alpha='asymmetric', eta=0.1, name='bad_reviews')\n",
    "neutral_reviews_3 = train_display_save(tokens=neutral_reviews['tokens'], nb_samples=100000, num_topics=3, alpha='asymmetric', eta='symmetric', name='neutral_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_start=time.time()\n",
    "# Inputs\n",
    "tokens_good = random.sample(good_reviews['tokens'].values.tolist(),100000)\n",
    "# Create Dictionary\n",
    "id2word_good = corpora.Dictionary(tokens_good)\n",
    "# Term Document Frequency\n",
    "corpus_good = [id2word_good.doc2bow(text) for text in tokens_good]\n",
    "print(time.time()-int_start)\n",
    "\n",
    "# Build LDA model\n",
    "int_start=time.time()\n",
    "lda_model_good = gensim.models.LdaMulticore(corpus=corpus_good,\n",
    "                                       id2word=id2word_good,\n",
    "                                       num_topics=3,\n",
    "                                       alpha = 'asymmetric',\n",
    "                                       eta = 0.1,\n",
    "                                       random_state=3,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)\n",
    "print(time.time()-int_start)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model_good, corpus_good, id2word_good)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(lda_model_good, corpus_good, id2word_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_lda.cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_start=time.time()\n",
    "# Inputs\n",
    "tokens_bad = random.sample(bad_reviews['tokens'].values.tolist(),100000)\n",
    "# Create Dictionary\n",
    "id2word_bad = corpora.Dictionary(tokens_bad)\n",
    "# Term Document Frequency\n",
    "corpus_bad = [id2word_bad.doc2bow(text) for text in tokens_bad]\n",
    "print(time.time()-int_start)\n",
    "\n",
    "# Build LDA model\n",
    "int_start=time.time()\n",
    "lda_model_bad = gensim.models.LdaMulticore(corpus=corpus_bad,\n",
    "                                       id2word=id2word_bad,\n",
    "                                       num_topics=3,\n",
    "                                       alpha = 'asymmetric',\n",
    "                                       eta = 0.1,\n",
    "                                       random_state=3,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)\n",
    "print(time.time()-int_start)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model_bad, corpus_bad, id2word_bad)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_start=time.time()\n",
    "# Inputs\n",
    "tokens_neutral = random.sample(neutral_reviews['tokens'].values.tolist(),100000)\n",
    "# Create Dictionary\n",
    "id2word_neutral = corpora.Dictionary(tokens_neutral)\n",
    "# Term Document Frequency\n",
    "corpus_neutral = [id2word_neutral.doc2bow(text) for text in tokens_neutral]\n",
    "print(time.time()-int_start)\n",
    "\n",
    "# Build LDA model\n",
    "int_start=time.time()\n",
    "lda_model_neutral = gensim.models.LdaMulticore(corpus=corpus_neutral,\n",
    "                                       id2word=id2word_neutral,\n",
    "                                       num_topics=3,\n",
    "                                       alpha = 'asymmetric',\n",
    "                                       eta = 'symmetric',\n",
    "                                       random_state=3,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)\n",
    "print(time.time()-int_start)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model_neutral, corpus_neutral, id2word_neutral)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pickle.load(open(\"docs.pickle\", \"rb\"))\n",
    "\n",
    "d = corpora.Dictionary(docs)\n",
    "\n",
    "freq = pd.DataFrame(d.dfs.values(), index=d.dfs.keys(), columns=['freq'])\n",
    "freq.index.name = 'idx'\n",
    "freq = freq.reset_index()\n",
    "freq['token'] = freq['idx'].apply(lambda x:d[x])\n",
    "freq = freq.sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq[freq['freq']>500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "docs = random.sample(docs, 100000)\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(docs)\n",
    "pickle.dump(id2word, open( \"id2word.p\", \"wb\" ) )\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, text, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "nb_words = len(id2word)\n",
    "\n",
    "# Topics range\n",
    "min_topics = 6\n",
    "max_topics = 10\n",
    "step_size = 1\n",
    "# topics_range = range(min_topics, max_topics, step_size)\n",
    "topics_range = [8]\n",
    "# Alpha parameter\n",
    "# Added in the loop\n",
    "\n",
    "# Alpha\n",
    "alpha = [\n",
    "#     0.1, \n",
    "#     'symmetric',\n",
    "#     'asymmetric'\n",
    "]\n",
    "\n",
    "# Beta parameter\n",
    "beta = [\n",
    "    0.1, \n",
    "#     200/nb_words\n",
    "]\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [\n",
    "#     gensim.utils.ClippedCorpus(corpus, num_of_docs*0.05), \n",
    "#     gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "#     gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), \n",
    "    self.corpus\n",
    "]\n",
    "\n",
    "corpus_title = [\n",
    "#     '25% Corpus',\n",
    "#     '50% Corpus',\n",
    "#     '75% Corpus',\n",
    "    '100% Corpus'\n",
    "]\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*(len(alpha)+1)*len(topics_range)))\n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            alpha.append(50/k)\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, text=docs,\n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    pbar.update(1)\n",
    "    res = pd.DataFrame(model_results)\n",
    "    res = pd.DataFrame(model_results).sort_values(\"Coherence\", ascending=False)\n",
    "    res.to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = res.iloc[0]\n",
    "num_topics = best_param['Topics']\n",
    "alpha = best_param['Alpha']\n",
    "eta = best_param['Beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "int_start=time.time()\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       alpha = alpha,\n",
    "                                       eta = eta,\n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)\n",
    "print(time.time()-int_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save('lda_test.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('reviews_concat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc['2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = list(tqdm.tqdm(preprocess(df.values.tolist(), stop_words), position=0, leave=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(docs, open( \"docs.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = pickle.load(open(\"docs.p\", \"rb\"))\n",
    "# id2word = pickle.load(open(\"id2word.p\", \"rb\"))\n",
    "\n",
    "# # Term Document Frequency\n",
    "# corpus = [id2word.doc2bow(text) for text in docs]\n",
    "\n",
    "# output = pd.concat([df.to_frame('description').reset_index(), pd.DataFrame(gensim.matutils.corpus2csc(lda_model.get_document_topics(corpus)).T.toarray(), columns=['topic_'+str(i) for i in range(1,num_topics+1)])], axis=1, ignore_index=True)\n",
    "\n",
    "# output.to_pickle('reviews_w_topics_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the bigram and trigram models\n",
    "# bigram = gensim.models.Phrases(data_words, min_count=5, threshold=150) # higher threshold fewer phrases.\n",
    "# # trigram = gensim.models.Phrases(bigram[data_words], threshold=150)\n",
    "\n",
    "# # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# # trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "#     texts_out = []\n",
    "#     for sent in texts:\n",
    "#         doc = nlp(\" \".join(sent)) \n",
    "#         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "#     return texts_out\n",
    "\n",
    "# # Remove Stop Words\n",
    "# data_words = remove_stopwords(data_words)\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_words = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# # Form Bigrams\n",
    "# data_words = make_bigrams(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build LDA model\n",
    "# lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "#                                        id2word=id2word,\n",
    "#                                        num_topics=8, \n",
    "#                                        random_state=100,\n",
    "#                                        chunksize=100,\n",
    "#                                        passes=10,\n",
    "#                                        per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.988px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
